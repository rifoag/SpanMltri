{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "from BaseEncoder import BaseEncoder\n",
    "from DataLoader import read_examples_from_file, ReviewDataset\n",
    "\n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from SpanMltri import SpanMltri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using {} device\".format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_te_label(te_label_dict, tokens):\n",
    "    # input : \n",
    "    #     te_label_dict = Dict; Key : span range (string), Value :  term type label (ASPECT, SENTIMENT, O)\n",
    "    #     tokens = list of tokens from the sentence\n",
    "    # output : List of pair (phrase, term_type_label)\n",
    "    te_label_list = []\n",
    "    \n",
    "    for span_range in te_label_dict:\n",
    "        start_idx, end_idx = span_range.split('-')\n",
    "        start_idx, end_idx = int(start_idx), int(end_idx)\n",
    "        sentence = ' '.join(tokens[start_idx:end_idx+1])\n",
    "        te_label = te_label_dict[span_range]\n",
    "        te_label_list.append((sentence, te_label))\n",
    "    \n",
    "    return te_label_list\n",
    "\n",
    "def decode_relation_label(relation_dict, tokens):\n",
    "    # input : \n",
    "    #     relation = Dict; Key : span range pair (aspect_term_span_range, opinion_term_span_range), Value :  sentiment polarity label (POSITIVE, NEGATIVE, NEUTRAL)\n",
    "    #     tokens = list of tokens from the sentence\n",
    "    # output : list of triples (aspect_term, opinion_term, polarity)\n",
    "    relation_list = []\n",
    "    \n",
    "    for span_range_pair in relation_dict:\n",
    "        aspect_term_span_range, opinion_term_span_range = span_range_pair\n",
    "        \n",
    "        aspect_term_start_idx, aspect_term_end_idx = aspect_term_span_range.split('-')\n",
    "        aspect_term_start_idx, aspect_term_end_idx = int(aspect_term_start_idx), int(aspect_term_end_idx)\n",
    "        aspect_term = ' '.join(tokens[aspect_term_start_idx:aspect_term_end_idx+1])\n",
    "        \n",
    "        opinion_term_start_idx, opinion_term_end_idx = opinion_term_span_range.split('-')\n",
    "        opinion_term_start_idx, opinion_term_end_idx = int(opinion_term_start_idx), int(opinion_term_end_idx)\n",
    "        opinion_term = ' '.join(tokens[opinion_term_start_idx:opinion_term_end_idx+1])\n",
    "        \n",
    "        relation_label = relation_dict[span_range_pair]\n",
    "        \n",
    "        relation_list.append((aspect_term, opinion_term, relation_label))\n",
    "    \n",
    "    return relation_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_FILE_PATH = \"dataset/train.tsv\"\n",
    "DEV_FILE_PATH = \"dataset/dev.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = ReviewDataset(TRAIN_FILE_PATH)\n",
    "dev_data = ReviewDataset(DEV_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['kamar', 'dibersihkan', 'dan', 'dirapikan', 'setiap', 'hari', ',', 'fasilitas', 'oke', ',', 'hanya', 'resepsionis', 'sering', 'tidak', 'ada', 'di', 'tempat', '.']\n",
      "[('fasilitas', 'ASPECT'), ('oke', 'SENTIMENT'), ('resepsionis', 'ASPECT'), ('sering tidak ada', 'SENTIMENT')]\n",
      "[('fasilitas', 'oke', 'PO'), ('resepsionis', 'sering tidak ada', 'NG')]\n"
     ]
    }
   ],
   "source": [
    "IDX = 2900\n",
    "print(train_data.texts[IDX])\n",
    "print(decode_te_label(train_data.te_label_dict[IDX], train_data.texts[IDX]))\n",
    "print(decode_relation_label(train_data.relation_dict[IDX], train_data.texts[IDX]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sudah', '2', 'kali', 'menginap', 'disana', 'dan', 'pelayanannya', 'memuaskan', '.']\n",
      "[('pelayanannya', 'ASPECT'), ('memuaskan', 'SENTIMENT')]\n",
      "[('pelayanannya', 'memuaskan', 'PO')]\n"
     ]
    }
   ],
   "source": [
    "IDX = 900\n",
    "print(dev_data.texts[IDX])\n",
    "print(decode_te_label(dev_data.te_label_dict[IDX], dev_data.texts[IDX]))\n",
    "print(decode_relation_label(dev_data.relation_dict[IDX], dev_data.texts[IDX]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SpanMltri().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "dev_dataloader = DataLoader(dev_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "\n",
    "lambda_t = 0.5\n",
    "lambda_r = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dataloader, train_data, model, loss_fn, optimizer):\n",
    "    size = len(train_dataloader.dataset)\n",
    "\n",
    "    for batch, X in enumerate(train_dataloader):\n",
    "        X = X.to(device)\n",
    "        current_te_label_dict = train_data.te_label_dict[(batch)*BATCH_SIZE:(batch+1)*BATCH_SIZE]\n",
    "        current_relation_dict = train_data.relation_dict[(batch)*BATCH_SIZE:(batch+1)*BATCH_SIZE]\n",
    "\n",
    "        logits_term_scorer, span_ranges, logits_relation_scorer, span_pair_ranges = model(X)\n",
    "\n",
    "        y_te_true = []\n",
    "        CURRENT_BATCH_SIZE = min(len(current_te_label_dict), BATCH_SIZE)\n",
    "        for i in range(CURRENT_BATCH_SIZE):\n",
    "            y_ = []\n",
    "            for span_range in span_ranges:\n",
    "                if span_range in current_te_label_dict[i]:\n",
    "                    label = current_te_label_dict[i][span_range]\n",
    "                    if label == 'ASPECT':\n",
    "                        y_.append(1)\n",
    "                    elif label == 'SENTIMENT':\n",
    "                        y_.append(2)\n",
    "                else: # label is O\n",
    "                    y_.append(0)        \n",
    "            y_te_true.append(torch.Tensor(y_))\n",
    "        y_te_true = torch.stack(y_te_true)\n",
    "        y_te_true = y_te_true.to(torch.long)\n",
    "\n",
    "        logits_term_scorer = logits_term_scorer.reshape(logits_term_scorer.shape[0]*logits_term_scorer.shape[1], logits_term_scorer.shape[-1])\n",
    "        y_te_true = y_te_true.reshape(-1).to(device)\n",
    "        te_loss = loss_fn(logits_term_scorer, y_te_true)\n",
    "\n",
    "        y_paote_true = []\n",
    "        CURRENT_BATCH_SIZE = min(len(current_relation_dict), BATCH_SIZE)\n",
    "        for i in range(CURRENT_BATCH_SIZE):\n",
    "            y_ = []\n",
    "            for span_pair_range in span_pair_ranges[i]:\n",
    "                if span_pair_range not in current_relation_dict[i]:\n",
    "                    y_.append(0)\n",
    "                else:\n",
    "                    label = current_relation_dict[i][span_pair_range]\n",
    "                    if label == 'PO':\n",
    "                        y_.append(1)\n",
    "                    elif label == 'NG':\n",
    "                        y_.append(2)\n",
    "                    elif label == 'NT':\n",
    "                        y_.append(3)\n",
    "            y_paote_true.append(torch.Tensor(y_))\n",
    "        y_paote_true = torch.stack(y_paote_true)\n",
    "        y_paote_true = y_paote_true.to(torch.long)\n",
    "\n",
    "        logits_relation_scorer = logits_relation_scorer.reshape(logits_relation_scorer.shape[0]*logits_relation_scorer.shape[1], logits_relation_scorer.shape[-1])\n",
    "        y_paote_true = y_paote_true.reshape(-1).to(device)\n",
    "        paote_loss = loss_fn(logits_relation_scorer, y_paote_true)\n",
    "\n",
    "        total_loss = lambda_t*te_loss + lambda_r*paote_loss\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 2 == 0:\n",
    "            total_loss, current = total_loss.item(), batch * len(X)\n",
    "            print(f\"loss: {total_loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dev_dataloader, model):\n",
    "    size = len(dev_dataloader.dataset)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dev_dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= size\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 1.333529  [    0/ 3000]\n",
      "loss: 1.280262  [   16/ 3000]\n",
      "loss: 1.256137  [   32/ 3000]\n",
      "loss: 1.234200  [   48/ 3000]\n",
      "loss: 1.231892  [   64/ 3000]\n",
      "loss: 1.233439  [   80/ 3000]\n",
      "loss: 1.228324  [   96/ 3000]\n",
      "loss: 1.226486  [  112/ 3000]\n",
      "loss: 1.234813  [  128/ 3000]\n",
      "loss: 1.232931  [  144/ 3000]\n",
      "loss: 1.229365  [  160/ 3000]\n",
      "loss: 1.226102  [  176/ 3000]\n",
      "loss: 1.225214  [  192/ 3000]\n",
      "loss: 1.230003  [  208/ 3000]\n",
      "loss: 1.223525  [  224/ 3000]\n",
      "loss: 1.222108  [  240/ 3000]\n",
      "loss: 1.225379  [  256/ 3000]\n",
      "loss: 1.223016  [  272/ 3000]\n",
      "loss: 1.225428  [  288/ 3000]\n",
      "loss: 1.214058  [  304/ 3000]\n",
      "loss: 1.226027  [  320/ 3000]\n",
      "loss: 1.226262  [  336/ 3000]\n",
      "loss: 1.218978  [  352/ 3000]\n",
      "loss: 1.224049  [  368/ 3000]\n",
      "loss: 1.220452  [  384/ 3000]\n",
      "loss: 1.225622  [  400/ 3000]\n",
      "loss: 1.221791  [  416/ 3000]\n",
      "loss: 1.220468  [  432/ 3000]\n",
      "loss: 1.214044  [  448/ 3000]\n",
      "loss: 1.226375  [  464/ 3000]\n",
      "loss: 1.216253  [  480/ 3000]\n",
      "loss: 1.211061  [  496/ 3000]\n",
      "loss: 1.218278  [  512/ 3000]\n",
      "loss: 1.214554  [  528/ 3000]\n",
      "loss: 1.196763  [  544/ 3000]\n",
      "loss: 1.179541  [  560/ 3000]\n",
      "loss: 1.182168  [  576/ 3000]\n",
      "loss: 1.190417  [  592/ 3000]\n",
      "loss: 1.173621  [  608/ 3000]\n",
      "loss: 1.172791  [  624/ 3000]\n",
      "loss: 1.173267  [  640/ 3000]\n",
      "loss: 1.173290  [  656/ 3000]\n",
      "loss: 1.171774  [  672/ 3000]\n",
      "loss: 1.171749  [  688/ 3000]\n",
      "loss: 1.172089  [  704/ 3000]\n",
      "loss: 1.171757  [  720/ 3000]\n",
      "loss: 1.171872  [  736/ 3000]\n",
      "loss: 1.172649  [  752/ 3000]\n",
      "loss: 1.172509  [  768/ 3000]\n",
      "loss: 1.172961  [  784/ 3000]\n",
      "loss: 1.172824  [  800/ 3000]\n",
      "loss: 1.172065  [  816/ 3000]\n",
      "loss: 1.172064  [  832/ 3000]\n",
      "loss: 1.171667  [  848/ 3000]\n",
      "loss: 1.171981  [  864/ 3000]\n",
      "loss: 1.171795  [  880/ 3000]\n",
      "loss: 1.171727  [  896/ 3000]\n",
      "loss: 1.172622  [  912/ 3000]\n",
      "loss: 1.172317  [  928/ 3000]\n",
      "loss: 1.172618  [  944/ 3000]\n",
      "loss: 1.172557  [  960/ 3000]\n",
      "loss: 1.171402  [  976/ 3000]\n",
      "loss: 1.172756  [  992/ 3000]\n",
      "loss: 1.171754  [ 1008/ 3000]\n",
      "loss: 1.171706  [ 1024/ 3000]\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, train_data, model, loss_fn, optimizer)\n",
    "    test(dev_dataloader, model)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
