{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from BaseEncoder import BaseEncoder\n",
    "from DataLoader import read_examples_from_file, ReviewDataset\n",
    "\n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from SpanMltri import SpanMltri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using {} device\".format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_te_label(te_label_dict, tokens):\n",
    "    # input : \n",
    "    #     te_label_dict = Dict; Key : span range (string), Value :  term type label (ASPECT, SENTIMENT, O)\n",
    "    #     tokens = list of tokens from the sentence\n",
    "    # output : List of pair (phrase, term_type_label)\n",
    "    te_label_list = []\n",
    "    \n",
    "    for span_range in te_label_dict:\n",
    "        start_idx, end_idx = span_range.split('-')\n",
    "        start_idx, end_idx = int(start_idx), int(end_idx)\n",
    "        sentence = ' '.join(tokens[start_idx:end_idx+1])\n",
    "        te_label = te_label_dict[span_range]\n",
    "        te_label_list.append((sentence, te_label))\n",
    "    \n",
    "    return te_label_list\n",
    "\n",
    "def decode_relation_label(relation_dict, tokens):\n",
    "    # input : \n",
    "    #     relation = Dict; Key : span range pair (aspect_term_span_range, opinion_term_span_range), Value :  sentiment polarity label (POSITIVE, NEGATIVE, NEUTRAL)\n",
    "    #     tokens = list of tokens from the sentence\n",
    "    # output : list of triples (aspect_term, opinion_term, polarity)\n",
    "    relation_list = []\n",
    "    \n",
    "    for span_range_pair in relation_dict:\n",
    "        aspect_term_span_range, opinion_term_span_range = span_range_pair\n",
    "        \n",
    "        aspect_term_start_idx, aspect_term_end_idx = aspect_term_span_range.split('-')\n",
    "        aspect_term_start_idx, aspect_term_end_idx = int(aspect_term_start_idx), int(aspect_term_end_idx)\n",
    "        aspect_term = ' '.join(tokens[aspect_term_start_idx:aspect_term_end_idx+1])\n",
    "        \n",
    "        opinion_term_start_idx, opinion_term_end_idx = opinion_term_span_range.split('-')\n",
    "        opinion_term_start_idx, opinion_term_end_idx = int(opinion_term_start_idx), int(opinion_term_end_idx)\n",
    "        opinion_term = ' '.join(tokens[opinion_term_start_idx:opinion_term_end_idx+1])\n",
    "        \n",
    "        relation_label = relation_dict[span_range_pair]\n",
    "        \n",
    "        relation_list.append((aspect_term, opinion_term, relation_label))\n",
    "    \n",
    "    return relation_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_FILE_PATH = \"dataset/train.tsv\"\n",
    "DEV_FILE_PATH = \"dataset/dev.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at indolem/indobert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at indolem/indobert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "train_data = ReviewDataset(TRAIN_FILE_PATH)\n",
    "dev_data = ReviewDataset(DEV_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['kain', 'kasur', 'selama', '4', 'malam', 'menginap', 'tidak', 'pernah', 'di', 'ganti', ',', 'dan', 'ac', 'kurang', 'dingin', '.']\n",
      "[('kasur selama', 'ASPECT'), ('pernah di ganti ,', 'SENTIMENT'), ('kurang', 'ASPECT'), ('dingin .', 'SENTIMENT')]\n",
      "[('kasur selama', 'pernah di ganti ,', 'NG'), ('kurang', 'dingin .', 'NG')]\n"
     ]
    }
   ],
   "source": [
    "IDX = 393\n",
    "print(train_data.texts[IDX])\n",
    "print(decode_te_label(train_data.te_label_dict[IDX], train_data.texts[IDX]))\n",
    "print(decode_relation_label(train_data.relation_dict[IDX], train_data.texts[IDX]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sebenarnya', 'kamar', 'baik', 'tetapi', 'kurang', 'profesional', ',', 'di', 'kasih', 'tahu', 'sarapan', 'nasi', 'goreng', ',', 'minta', 'di', 'antar', 'jam', '8.30', 'ehh', 'datang', 'jam', '9', 'itu', 'pun', 'di', 'telepon', 'dulu', 'dan', 'yang', 'datang', 'bahkan', 'mie', 'muah', 'telor', '.', 'hm', '.']\n",
      "[('baik', 'ASPECT'), ('tetapi', 'SENTIMENT'), ('profesional ,', 'SENTIMENT')]\n",
      "[('baik', 'tetapi', 'PO')]\n"
     ]
    }
   ],
   "source": [
    "IDX = 38\n",
    "print(dev_data.texts[IDX])\n",
    "print(decode_te_label(dev_data.te_label_dict[IDX], dev_data.texts[IDX]))\n",
    "print(decode_relation_label(dev_data.relation_dict[IDX], dev_data.texts[IDX]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at indolem/indobert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = SpanMltri().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "dev_dataloader = DataLoader(dev_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "\n",
    "lambda_t = 0.5\n",
    "lambda_r = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dataloader, train_data, model, loss_fn, optimizer):\n",
    "    size = len(train_dataloader.dataset)\n",
    "\n",
    "    for batch, X in enumerate(train_dataloader):\n",
    "        X = X.to(device)\n",
    "        current_te_label_dict = train_data.te_label_dict[(batch)*BATCH_SIZE:(batch+1)*BATCH_SIZE]\n",
    "        current_relation_dict = train_data.relation_dict[(batch)*BATCH_SIZE:(batch+1)*BATCH_SIZE]\n",
    "        sentences = train_data.texts[(batch)*BATCH_SIZE:(batch+1)*BATCH_SIZE]\n",
    "        logits_term_scorer, span_ranges, logits_relation_scorer, span_pair_ranges = model(X)\n",
    "\n",
    "        y_te_true = []\n",
    "        CURRENT_BATCH_SIZE = min(len(current_te_label_dict), BATCH_SIZE)\n",
    "        for i in range(CURRENT_BATCH_SIZE):\n",
    "            y_ = []\n",
    "            for span_range in span_ranges:\n",
    "                if span_range in current_te_label_dict[i]:\n",
    "                    label = current_te_label_dict[i][span_range]\n",
    "                    if label == 'ASPECT':\n",
    "                        y_.append(1)\n",
    "                    elif label == 'SENTIMENT':\n",
    "                        y_.append(2)\n",
    "                else: # label is O\n",
    "                    y_.append(0)        \n",
    "            y_te_true.append(torch.Tensor(y_))\n",
    "        y_te_true = torch.stack(y_te_true)\n",
    "        y_te_true = y_te_true.to(torch.long)\n",
    "\n",
    "        logits_term_scorer = logits_term_scorer.reshape(logits_term_scorer.shape[0]*logits_term_scorer.shape[1], logits_term_scorer.shape[-1])\n",
    "        y_te_true = y_te_true.reshape(-1).to(device)\n",
    "        te_loss = loss_fn(logits_term_scorer, y_te_true)\n",
    "\n",
    "        y_paote_true = []\n",
    "        CURRENT_BATCH_SIZE = min(len(current_relation_dict), BATCH_SIZE)\n",
    "        for i in range(CURRENT_BATCH_SIZE):\n",
    "            y_ = []\n",
    "            for span_pair_range in span_pair_ranges[i]:\n",
    "                if span_pair_range not in current_relation_dict[i]:\n",
    "                    y_.append(0)\n",
    "                else:\n",
    "                    label = current_relation_dict[i][span_pair_range]\n",
    "                    if label == 'PO':\n",
    "                        y_.append(1)\n",
    "                    elif label == 'NG':\n",
    "                        y_.append(2)\n",
    "                    elif label == 'NT':\n",
    "                        y_.append(3)\n",
    "            y_paote_true.append(torch.Tensor(y_))\n",
    "        y_paote_true = torch.stack(y_paote_true)\n",
    "        y_paote_true = y_paote_true.to(torch.long)\n",
    "\n",
    "        logits_relation_scorer = logits_relation_scorer.reshape(logits_relation_scorer.shape[0]*logits_relation_scorer.shape[1], logits_relation_scorer.shape[-1])\n",
    "        y_paote_true = y_paote_true.reshape(-1).to(device)\n",
    "        paote_loss = loss_fn(logits_relation_scorer, y_paote_true)\n",
    "\n",
    "        total_loss = lambda_t*te_loss + lambda_r*paote_loss\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 2 == 0:\n",
    "            total_loss, current = total_loss.item(), batch * len(X)\n",
    "            print(f\"loss: {total_loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dev_dataloader, model):\n",
    "    size = len(dev_dataloader.dataset)\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch, X in enumerate(dev_dataloader):\n",
    "            X = X.to(device)\n",
    "            \n",
    "            current_te_label_dict = dev_data.te_label_dict[(batch)*BATCH_SIZE:(batch+1)*BATCH_SIZE]\n",
    "            current_relation_dict = dev_data.relation_dict[(batch)*BATCH_SIZE:(batch+1)*BATCH_SIZE]\n",
    "            \n",
    "            logits_term_scorer, span_ranges, logits_relation_scorer, span_pair_ranges = model(X)\n",
    "\n",
    "            y_te_true = []\n",
    "            CURRENT_BATCH_SIZE = min(len(current_te_label_dict), BATCH_SIZE)\n",
    "            for i in range(CURRENT_BATCH_SIZE):\n",
    "                y_ = []\n",
    "                for span_range in span_ranges:\n",
    "                    if span_range in current_te_label_dict[i]:\n",
    "                        label = current_te_label_dict[i][span_range]\n",
    "                        if label == 'ASPECT':\n",
    "                            y_.append(1)\n",
    "                        elif label == 'SENTIMENT':\n",
    "                            y_.append(2)\n",
    "                    else: # label is O\n",
    "                        y_.append(0)        \n",
    "                y_te_true.append(torch.Tensor(y_))\n",
    "            y_te_true = torch.stack(y_te_true)\n",
    "            y_te_true = y_te_true.to(torch.long)\n",
    "\n",
    "            logits_term_scorer = logits_term_scorer.reshape(logits_term_scorer.shape[0]*logits_term_scorer.shape[1], logits_term_scorer.shape[-1])\n",
    "            y_te_true = y_te_true.reshape(-1).to(device)\n",
    "            te_loss = loss_fn(logits_term_scorer, y_te_true)\n",
    "\n",
    "            y_paote_true = []\n",
    "            CURRENT_BATCH_SIZE = min(len(current_relation_dict), BATCH_SIZE)\n",
    "            for i in range(CURRENT_BATCH_SIZE):\n",
    "                y_ = []\n",
    "                for span_pair_range in span_pair_ranges[i]:\n",
    "                    if span_pair_range not in current_relation_dict[i]:\n",
    "                        y_.append(0)\n",
    "                    else:\n",
    "                        label = current_relation_dict[i][span_pair_range]\n",
    "                        if label == 'PO':\n",
    "                            y_.append(1)\n",
    "                        elif label == 'NG':\n",
    "                            y_.append(2)\n",
    "                        elif label == 'NT':\n",
    "                            y_.append(3)\n",
    "                y_paote_true.append(torch.Tensor(y_))\n",
    "            y_paote_true = torch.stack(y_paote_true)\n",
    "            y_paote_true = y_paote_true.to(torch.long)\n",
    "\n",
    "            logits_relation_scorer = logits_relation_scorer.reshape(logits_relation_scorer.shape[0]*logits_relation_scorer.shape[1], logits_relation_scorer.shape[-1])\n",
    "            y_paote_true = y_paote_true.reshape(-1).to(device)\n",
    "            paote_loss = loss_fn(logits_relation_scorer, y_paote_true)\n",
    "\n",
    "            total_loss += lambda_t*te_loss.item() + lambda_r*paote_loss.item()\n",
    "\n",
    "    total_loss /= size\n",
    "    print(f\"Test Error: \\n Avg loss: {total_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 16.687113  [    0/ 3000]\n",
      "loss: 2.232355  [   32/ 3000]\n",
      "loss: 0.308499  [   64/ 3000]\n",
      "loss: 0.123945  [   96/ 3000]\n",
      "loss: 0.058107  [  128/ 3000]\n",
      "loss: 0.043705  [  160/ 3000]\n",
      "loss: 0.040378  [  192/ 3000]\n",
      "loss: 0.041156  [  224/ 3000]\n",
      "loss: 0.034098  [  256/ 3000]\n",
      "loss: 0.030528  [  288/ 3000]\n",
      "loss: 0.038016  [  320/ 3000]\n",
      "loss: 0.031810  [  352/ 3000]\n",
      "loss: 0.035299  [  384/ 3000]\n",
      "loss: 0.033669  [  416/ 3000]\n",
      "loss: 0.023997  [  448/ 3000]\n",
      "loss: 0.037645  [  480/ 3000]\n",
      "loss: 0.027837  [  512/ 3000]\n",
      "loss: 0.034670  [  544/ 3000]\n",
      "loss: 0.033726  [  576/ 3000]\n",
      "loss: 0.033039  [  608/ 3000]\n",
      "loss: 0.028358  [  640/ 3000]\n",
      "loss: 0.027258  [  672/ 3000]\n",
      "loss: 0.031088  [  704/ 3000]\n",
      "loss: 0.028634  [  736/ 3000]\n",
      "loss: 0.033256  [  768/ 3000]\n",
      "loss: 0.035022  [  800/ 3000]\n",
      "loss: 0.032138  [  832/ 3000]\n",
      "loss: 0.030711  [  864/ 3000]\n",
      "loss: 0.032198  [  896/ 3000]\n",
      "loss: 0.036880  [  928/ 3000]\n",
      "loss: 0.034458  [  960/ 3000]\n",
      "loss: 0.037017  [  992/ 3000]\n",
      "loss: 0.030382  [ 1024/ 3000]\n",
      "loss: 0.024401  [ 1056/ 3000]\n",
      "loss: 0.030115  [ 1088/ 3000]\n",
      "loss: 0.024992  [ 1120/ 3000]\n",
      "loss: 0.023150  [ 1152/ 3000]\n",
      "loss: 0.031499  [ 1184/ 3000]\n",
      "loss: 0.030933  [ 1216/ 3000]\n",
      "loss: 0.025967  [ 1248/ 3000]\n",
      "loss: 0.037662  [ 1280/ 3000]\n",
      "loss: 0.040368  [ 1312/ 3000]\n",
      "loss: 0.031427  [ 1344/ 3000]\n",
      "loss: 0.035158  [ 1376/ 3000]\n",
      "loss: 0.032468  [ 1408/ 3000]\n",
      "loss: 0.029746  [ 1440/ 3000]\n",
      "loss: 0.028566  [ 1472/ 3000]\n",
      "loss: 0.024759  [ 1504/ 3000]\n",
      "loss: 0.029241  [ 1536/ 3000]\n",
      "loss: 0.025943  [ 1568/ 3000]\n",
      "loss: 0.031424  [ 1600/ 3000]\n",
      "loss: 0.026369  [ 1632/ 3000]\n",
      "loss: 0.035291  [ 1664/ 3000]\n",
      "loss: 0.034045  [ 1696/ 3000]\n",
      "loss: 0.025414  [ 1728/ 3000]\n",
      "loss: 0.026556  [ 1760/ 3000]\n",
      "loss: 0.030552  [ 1792/ 3000]\n",
      "loss: 0.027060  [ 1824/ 3000]\n",
      "loss: 0.023590  [ 1856/ 3000]\n",
      "loss: 0.027409  [ 1888/ 3000]\n",
      "loss: 0.030539  [ 1920/ 3000]\n",
      "loss: 0.029189  [ 1952/ 3000]\n",
      "loss: 0.028198  [ 1984/ 3000]\n",
      "loss: 0.027921  [ 2016/ 3000]\n",
      "loss: 0.024785  [ 2048/ 3000]\n",
      "loss: 0.032618  [ 2080/ 3000]\n",
      "loss: 0.025362  [ 2112/ 3000]\n",
      "loss: 0.033505  [ 2144/ 3000]\n",
      "loss: 0.026658  [ 2176/ 3000]\n",
      "loss: 0.022110  [ 2208/ 3000]\n",
      "loss: 0.028719  [ 2240/ 3000]\n",
      "loss: 0.023517  [ 2272/ 3000]\n",
      "loss: 0.021676  [ 2304/ 3000]\n",
      "loss: 0.031806  [ 2336/ 3000]\n",
      "loss: 0.026848  [ 2368/ 3000]\n",
      "loss: 0.022508  [ 2400/ 3000]\n",
      "loss: 0.025726  [ 2432/ 3000]\n",
      "loss: 0.027722  [ 2464/ 3000]\n",
      "loss: 0.020269  [ 2496/ 3000]\n",
      "loss: 0.025646  [ 2528/ 3000]\n",
      "loss: 0.037014  [ 2560/ 3000]\n",
      "loss: 0.024112  [ 2592/ 3000]\n",
      "loss: 0.024652  [ 2624/ 3000]\n",
      "loss: 0.024035  [ 2656/ 3000]\n",
      "loss: 0.026911  [ 2688/ 3000]\n",
      "loss: 0.027392  [ 2720/ 3000]\n",
      "loss: 0.019516  [ 2752/ 3000]\n",
      "loss: 0.023204  [ 2784/ 3000]\n",
      "loss: 0.024902  [ 2816/ 3000]\n",
      "loss: 0.023385  [ 2848/ 3000]\n",
      "loss: 0.030086  [ 2880/ 3000]\n",
      "loss: 0.023418  [ 2912/ 3000]\n",
      "loss: 0.019053  [ 2944/ 3000]\n",
      "loss: 0.020926  [ 2976/ 3000]\n",
      "Test Error: \n",
      " Avg loss: 0.001594 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.023309  [    0/ 3000]\n",
      "loss: 0.027112  [   32/ 3000]\n",
      "loss: 0.024224  [   64/ 3000]\n",
      "loss: 0.032631  [   96/ 3000]\n",
      "loss: 0.025816  [  128/ 3000]\n",
      "loss: 0.020547  [  160/ 3000]\n",
      "loss: 0.027850  [  192/ 3000]\n",
      "loss: 0.031012  [  224/ 3000]\n",
      "loss: 0.024619  [  256/ 3000]\n",
      "loss: 0.021377  [  288/ 3000]\n",
      "loss: 0.029966  [  320/ 3000]\n",
      "loss: 0.024402  [  352/ 3000]\n",
      "loss: 0.026880  [  384/ 3000]\n",
      "loss: 0.025507  [  416/ 3000]\n",
      "loss: 0.018898  [  448/ 3000]\n",
      "loss: 0.029696  [  480/ 3000]\n",
      "loss: 0.021322  [  512/ 3000]\n",
      "loss: 0.027867  [  544/ 3000]\n",
      "loss: 0.027611  [  576/ 3000]\n",
      "loss: 0.025680  [  608/ 3000]\n",
      "loss: 0.022134  [  640/ 3000]\n",
      "loss: 0.020265  [  672/ 3000]\n",
      "loss: 0.023450  [  704/ 3000]\n",
      "loss: 0.021559  [  736/ 3000]\n",
      "loss: 0.025815  [  768/ 3000]\n",
      "loss: 0.027929  [  800/ 3000]\n",
      "loss: 0.023675  [  832/ 3000]\n",
      "loss: 0.023126  [  864/ 3000]\n",
      "loss: 0.025793  [  896/ 3000]\n",
      "loss: 0.029188  [  928/ 3000]\n",
      "loss: 0.026972  [  960/ 3000]\n",
      "loss: 0.029649  [  992/ 3000]\n",
      "loss: 0.022034  [ 1024/ 3000]\n",
      "loss: 0.018093  [ 1056/ 3000]\n",
      "loss: 0.024156  [ 1088/ 3000]\n",
      "loss: 0.017342  [ 1120/ 3000]\n",
      "loss: 0.017158  [ 1152/ 3000]\n",
      "loss: 0.025137  [ 1184/ 3000]\n",
      "loss: 0.023940  [ 1216/ 3000]\n",
      "loss: 0.019098  [ 1248/ 3000]\n",
      "loss: 0.030944  [ 1280/ 3000]\n",
      "loss: 0.032466  [ 1312/ 3000]\n",
      "loss: 0.025177  [ 1344/ 3000]\n",
      "loss: 0.029180  [ 1376/ 3000]\n",
      "loss: 0.026999  [ 1408/ 3000]\n",
      "loss: 0.024492  [ 1440/ 3000]\n",
      "loss: 0.022768  [ 1472/ 3000]\n",
      "loss: 0.019272  [ 1504/ 3000]\n",
      "loss: 0.024063  [ 1536/ 3000]\n",
      "loss: 0.020336  [ 1568/ 3000]\n",
      "loss: 0.026692  [ 1600/ 3000]\n",
      "loss: 0.020279  [ 1632/ 3000]\n",
      "loss: 0.029973  [ 1664/ 3000]\n",
      "loss: 0.028682  [ 1696/ 3000]\n",
      "loss: 0.020485  [ 1728/ 3000]\n",
      "loss: 0.021146  [ 1760/ 3000]\n",
      "loss: 0.025386  [ 1792/ 3000]\n",
      "loss: 0.022235  [ 1824/ 3000]\n",
      "loss: 0.018909  [ 1856/ 3000]\n",
      "loss: 0.022254  [ 1888/ 3000]\n",
      "loss: 0.025550  [ 1920/ 3000]\n",
      "loss: 0.024813  [ 1952/ 3000]\n",
      "loss: 0.023685  [ 1984/ 3000]\n",
      "loss: 0.023452  [ 2016/ 3000]\n",
      "loss: 0.020369  [ 2048/ 3000]\n",
      "loss: 0.027572  [ 2080/ 3000]\n",
      "loss: 0.020638  [ 2112/ 3000]\n",
      "loss: 0.029560  [ 2144/ 3000]\n",
      "loss: 0.021702  [ 2176/ 3000]\n",
      "loss: 0.018125  [ 2208/ 3000]\n",
      "loss: 0.024338  [ 2240/ 3000]\n",
      "loss: 0.019797  [ 2272/ 3000]\n",
      "loss: 0.018367  [ 2304/ 3000]\n",
      "loss: 0.028363  [ 2336/ 3000]\n",
      "loss: 0.023438  [ 2368/ 3000]\n",
      "loss: 0.019105  [ 2400/ 3000]\n",
      "loss: 0.021539  [ 2432/ 3000]\n",
      "loss: 0.024124  [ 2464/ 3000]\n",
      "loss: 0.016932  [ 2496/ 3000]\n",
      "loss: 0.021759  [ 2528/ 3000]\n",
      "loss: 0.033286  [ 2560/ 3000]\n",
      "loss: 0.020979  [ 2592/ 3000]\n",
      "loss: 0.021582  [ 2624/ 3000]\n",
      "loss: 0.020578  [ 2656/ 3000]\n",
      "loss: 0.024017  [ 2688/ 3000]\n",
      "loss: 0.023630  [ 2720/ 3000]\n",
      "loss: 0.016695  [ 2752/ 3000]\n",
      "loss: 0.020431  [ 2784/ 3000]\n",
      "loss: 0.021647  [ 2816/ 3000]\n",
      "loss: 0.020199  [ 2848/ 3000]\n",
      "loss: 0.027413  [ 2880/ 3000]\n",
      "loss: 0.020699  [ 2912/ 3000]\n",
      "loss: 0.015611  [ 2944/ 3000]\n",
      "loss: 0.017594  [ 2976/ 3000]\n",
      "Test Error: \n",
      " Avg loss: 0.001410 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.020618  [    0/ 3000]\n",
      "loss: 0.023530  [   32/ 3000]\n",
      "loss: 0.021767  [   64/ 3000]\n",
      "loss: 0.029824  [   96/ 3000]\n",
      "loss: 0.023042  [  128/ 3000]\n",
      "loss: 0.017036  [  160/ 3000]\n",
      "loss: 0.025118  [  192/ 3000]\n",
      "loss: 0.027808  [  224/ 3000]\n",
      "loss: 0.022319  [  256/ 3000]\n",
      "loss: 0.019390  [  288/ 3000]\n",
      "loss: 0.027289  [  320/ 3000]\n",
      "loss: 0.021812  [  352/ 3000]\n",
      "loss: 0.024566  [  384/ 3000]\n",
      "loss: 0.021837  [  416/ 3000]\n",
      "loss: 0.016978  [  448/ 3000]\n",
      "loss: 0.026928  [  480/ 3000]\n",
      "loss: 0.019430  [  512/ 3000]\n",
      "loss: 0.025307  [  544/ 3000]\n",
      "loss: 0.024547  [  576/ 3000]\n",
      "loss: 0.022967  [  608/ 3000]\n",
      "loss: 0.020319  [  640/ 3000]\n",
      "loss: 0.018401  [  672/ 3000]\n",
      "loss: 0.021084  [  704/ 3000]\n",
      "loss: 0.019139  [  736/ 3000]\n",
      "loss: 0.023016  [  768/ 3000]\n",
      "loss: 0.025844  [  800/ 3000]\n",
      "loss: 0.021546  [  832/ 3000]\n",
      "loss: 0.020388  [  864/ 3000]\n",
      "loss: 0.023716  [  896/ 3000]\n",
      "loss: 0.026989  [  928/ 3000]\n",
      "loss: 0.024754  [  960/ 3000]\n",
      "loss: 0.027219  [  992/ 3000]\n",
      "loss: 0.019236  [ 1024/ 3000]\n",
      "loss: 0.015996  [ 1056/ 3000]\n",
      "loss: 0.022535  [ 1088/ 3000]\n",
      "loss: 0.015180  [ 1120/ 3000]\n",
      "loss: 0.014904  [ 1152/ 3000]\n",
      "loss: 0.022716  [ 1184/ 3000]\n",
      "loss: 0.021446  [ 1216/ 3000]\n",
      "loss: 0.017123  [ 1248/ 3000]\n",
      "loss: 0.027100  [ 1280/ 3000]\n",
      "loss: 0.029589  [ 1312/ 3000]\n",
      "loss: 0.023084  [ 1344/ 3000]\n",
      "loss: 0.026470  [ 1376/ 3000]\n",
      "loss: 0.024228  [ 1408/ 3000]\n",
      "loss: 0.022034  [ 1440/ 3000]\n",
      "loss: 0.019779  [ 1472/ 3000]\n",
      "loss: 0.016405  [ 1504/ 3000]\n",
      "loss: 0.021421  [ 1536/ 3000]\n",
      "loss: 0.017980  [ 1568/ 3000]\n",
      "loss: 0.023896  [ 1600/ 3000]\n",
      "loss: 0.017438  [ 1632/ 3000]\n",
      "loss: 0.027451  [ 1664/ 3000]\n",
      "loss: 0.026206  [ 1696/ 3000]\n",
      "loss: 0.017756  [ 1728/ 3000]\n",
      "loss: 0.018145  [ 1760/ 3000]\n",
      "loss: 0.022337  [ 1792/ 3000]\n",
      "loss: 0.019450  [ 1824/ 3000]\n",
      "loss: 0.016277  [ 1856/ 3000]\n",
      "loss: 0.019163  [ 1888/ 3000]\n",
      "loss: 0.022592  [ 1920/ 3000]\n",
      "loss: 0.022043  [ 1952/ 3000]\n",
      "loss: 0.020575  [ 1984/ 3000]\n",
      "loss: 0.020693  [ 2016/ 3000]\n",
      "loss: 0.018167  [ 2048/ 3000]\n",
      "loss: 0.024854  [ 2080/ 3000]\n",
      "loss: 0.018078  [ 2112/ 3000]\n",
      "loss: 0.025701  [ 2144/ 3000]\n",
      "loss: 0.018464  [ 2176/ 3000]\n",
      "loss: 0.015590  [ 2208/ 3000]\n",
      "loss: 0.020847  [ 2240/ 3000]\n",
      "loss: 0.017868  [ 2272/ 3000]\n",
      "loss: 0.015974  [ 2304/ 3000]\n",
      "loss: 0.025260  [ 2336/ 3000]\n",
      "loss: 0.020318  [ 2368/ 3000]\n",
      "loss: 0.016415  [ 2400/ 3000]\n",
      "loss: 0.018141  [ 2432/ 3000]\n",
      "loss: 0.021557  [ 2464/ 3000]\n",
      "loss: 0.013934  [ 2496/ 3000]\n",
      "loss: 0.018983  [ 2528/ 3000]\n",
      "loss: 0.029114  [ 2560/ 3000]\n",
      "loss: 0.018265  [ 2592/ 3000]\n",
      "loss: 0.018293  [ 2624/ 3000]\n",
      "loss: 0.017418  [ 2656/ 3000]\n",
      "loss: 0.019571  [ 2688/ 3000]\n",
      "loss: 0.020714  [ 2720/ 3000]\n",
      "loss: 0.013822  [ 2752/ 3000]\n",
      "loss: 0.017857  [ 2784/ 3000]\n",
      "loss: 0.018142  [ 2816/ 3000]\n",
      "loss: 0.017390  [ 2848/ 3000]\n",
      "loss: 0.024397  [ 2880/ 3000]\n",
      "loss: 0.017877  [ 2912/ 3000]\n",
      "loss: 0.012784  [ 2944/ 3000]\n",
      "loss: 0.014624  [ 2976/ 3000]\n",
      "Test Error: \n",
      " Avg loss: 0.001191 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.017146  [    0/ 3000]\n",
      "loss: 0.020026  [   32/ 3000]\n",
      "loss: 0.018533  [   64/ 3000]\n",
      "loss: 0.025958  [   96/ 3000]\n",
      "loss: 0.019430  [  128/ 3000]\n",
      "loss: 0.013525  [  160/ 3000]\n",
      "loss: 0.021097  [  192/ 3000]\n",
      "loss: 0.022711  [  224/ 3000]\n",
      "loss: 0.019365  [  256/ 3000]\n",
      "loss: 0.015738  [  288/ 3000]\n",
      "loss: 0.024317  [  320/ 3000]\n",
      "loss: 0.018828  [  352/ 3000]\n",
      "loss: 0.021357  [  384/ 3000]\n",
      "loss: 0.016951  [  416/ 3000]\n",
      "loss: 0.014664  [  448/ 3000]\n",
      "loss: 0.022600  [  480/ 3000]\n",
      "loss: 0.014821  [  512/ 3000]\n",
      "loss: 0.020971  [  544/ 3000]\n",
      "loss: 0.019816  [  576/ 3000]\n",
      "loss: 0.019497  [  608/ 3000]\n",
      "loss: 0.017486  [  640/ 3000]\n",
      "loss: 0.015159  [  672/ 3000]\n",
      "loss: 0.016928  [  704/ 3000]\n",
      "loss: 0.015780  [  736/ 3000]\n",
      "loss: 0.017909  [  768/ 3000]\n",
      "loss: 0.022026  [  800/ 3000]\n",
      "loss: 0.018428  [  832/ 3000]\n",
      "loss: 0.016140  [  864/ 3000]\n",
      "loss: 0.019810  [  896/ 3000]\n",
      "loss: 0.022864  [  928/ 3000]\n",
      "loss: 0.020423  [  960/ 3000]\n",
      "loss: 0.022255  [  992/ 3000]\n",
      "loss: 0.015000  [ 1024/ 3000]\n",
      "loss: 0.012936  [ 1056/ 3000]\n",
      "loss: 0.018800  [ 1088/ 3000]\n",
      "loss: 0.012079  [ 1120/ 3000]\n",
      "loss: 0.011586  [ 1152/ 3000]\n",
      "loss: 0.018245  [ 1184/ 3000]\n",
      "loss: 0.017717  [ 1216/ 3000]\n",
      "loss: 0.013830  [ 1248/ 3000]\n",
      "loss: 0.021440  [ 1280/ 3000]\n",
      "loss: 0.023535  [ 1312/ 3000]\n",
      "loss: 0.018335  [ 1344/ 3000]\n",
      "loss: 0.021132  [ 1376/ 3000]\n",
      "loss: 0.019899  [ 1408/ 3000]\n",
      "loss: 0.018027  [ 1440/ 3000]\n",
      "loss: 0.015306  [ 1472/ 3000]\n",
      "loss: 0.012539  [ 1504/ 3000]\n",
      "loss: 0.017601  [ 1536/ 3000]\n",
      "loss: 0.013775  [ 1568/ 3000]\n",
      "loss: 0.019115  [ 1600/ 3000]\n",
      "loss: 0.013061  [ 1632/ 3000]\n",
      "loss: 0.022524  [ 1664/ 3000]\n",
      "loss: 0.021450  [ 1696/ 3000]\n",
      "loss: 0.014223  [ 1728/ 3000]\n",
      "loss: 0.013581  [ 1760/ 3000]\n",
      "loss: 0.017910  [ 1792/ 3000]\n",
      "loss: 0.015572  [ 1824/ 3000]\n",
      "loss: 0.012972  [ 1856/ 3000]\n",
      "loss: 0.015124  [ 1888/ 3000]\n",
      "loss: 0.017490  [ 1920/ 3000]\n",
      "loss: 0.017693  [ 1952/ 3000]\n",
      "loss: 0.016264  [ 1984/ 3000]\n",
      "loss: 0.016138  [ 2016/ 3000]\n",
      "loss: 0.013826  [ 2048/ 3000]\n",
      "loss: 0.020048  [ 2080/ 3000]\n",
      "loss: 0.014353  [ 2112/ 3000]\n",
      "loss: 0.020533  [ 2144/ 3000]\n",
      "loss: 0.014655  [ 2176/ 3000]\n",
      "loss: 0.011837  [ 2208/ 3000]\n",
      "loss: 0.016122  [ 2240/ 3000]\n",
      "loss: 0.014309  [ 2272/ 3000]\n",
      "loss: 0.012782  [ 2304/ 3000]\n",
      "loss: 0.020201  [ 2336/ 3000]\n",
      "loss: 0.016036  [ 2368/ 3000]\n",
      "loss: 0.012666  [ 2400/ 3000]\n",
      "loss: 0.013933  [ 2432/ 3000]\n",
      "loss: 0.017351  [ 2464/ 3000]\n",
      "loss: 0.010952  [ 2496/ 3000]\n",
      "loss: 0.015338  [ 2528/ 3000]\n",
      "loss: 0.023694  [ 2560/ 3000]\n",
      "loss: 0.015377  [ 2592/ 3000]\n",
      "loss: 0.014802  [ 2624/ 3000]\n",
      "loss: 0.013815  [ 2656/ 3000]\n",
      "loss: 0.014955  [ 2688/ 3000]\n",
      "loss: 0.016332  [ 2720/ 3000]\n",
      "loss: 0.010841  [ 2752/ 3000]\n",
      "loss: 0.014794  [ 2784/ 3000]\n",
      "loss: 0.014339  [ 2816/ 3000]\n",
      "loss: 0.013577  [ 2848/ 3000]\n",
      "loss: 0.019673  [ 2880/ 3000]\n",
      "loss: 0.014282  [ 2912/ 3000]\n",
      "loss: 0.009681  [ 2944/ 3000]\n",
      "loss: 0.011263  [ 2976/ 3000]\n",
      "Test Error: \n",
      " Avg loss: 0.000960 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.014088  [    0/ 3000]\n",
      "loss: 0.016632  [   32/ 3000]\n",
      "loss: 0.015262  [   64/ 3000]\n",
      "loss: 0.021792  [   96/ 3000]\n",
      "loss: 0.015506  [  128/ 3000]\n",
      "loss: 0.010391  [  160/ 3000]\n",
      "loss: 0.017184  [  192/ 3000]\n",
      "loss: 0.018354  [  224/ 3000]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-5d2bb392a80a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Epoch {t+1}\\n-------------------------------\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdev_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Done!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-12bf90eeea54>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(train_dataloader, train_data, model, loss_fn, optimizer)\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[1;31m# Backpropagation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m         \u001b[0mtotal_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tesis\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 245\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tesis\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, train_data, model, loss_fn, optimizer)\n",
    "    test(dev_dataloader, model)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SpanMltri().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpanMltri(\n",
       "  (base_encoder): BaseEncoder(\n",
       "    (model): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(31923, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (3): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (4): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (5): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (6): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (7): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (8): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (9): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (10): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (11): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pooler): BertPooler(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (span_generator): SpanGenerator()\n",
       "  (term_scorer): TermScorer(\n",
       "    (linear_relu): Sequential(\n",
       "      (0): Linear(in_features=768, out_features=3, bias=True)\n",
       "      (1): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (relation_scorer): RelationScorer(\n",
       "    (span_scorer): Sequential(\n",
       "      (0): Linear(in_features=768, out_features=4, bias=True)\n",
       "      (1): ReLU()\n",
       "    )\n",
       "    (pair_scorer): Sequential(\n",
       "      (0): Linear(in_features=2304, out_features=4, bias=True)\n",
       "      (1): ReLU()\n",
       "    )\n",
       "    (final_softmax): Softmax(dim=-1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('model_weights.pth'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for batch, X in enumerate(dev_dataloader):\n",
    "        X = X.to(device)\n",
    "        \n",
    "        sentences = train_data.texts[(batch)*BATCH_SIZE:(batch+1)*BATCH_SIZE]\n",
    "        current_te_label_dict = train_data.te_label_dict[(batch)*BATCH_SIZE:(batch+1)*BATCH_SIZE]\n",
    "        current_relation_dict = train_data.relation_dict[(batch)*BATCH_SIZE:(batch+1)*BATCH_SIZE]\n",
    "        \n",
    "        logits_term_scorer, span_ranges, logits_relation_scorer, span_pair_ranges = model(X)\n",
    "        \n",
    "        y_te_true = []\n",
    "        CURRENT_BATCH_SIZE = min(len(current_te_label_dict), BATCH_SIZE)\n",
    "        for i in range(CURRENT_BATCH_SIZE):\n",
    "            y_ = []\n",
    "            for span_range in span_ranges:\n",
    "                if span_range in current_te_label_dict[i]:\n",
    "                    label = current_te_label_dict[i][span_range]\n",
    "                    if label == 'ASPECT':\n",
    "                        y_.append(1)\n",
    "                    elif label == 'SENTIMENT':\n",
    "                        y_.append(2)\n",
    "                else: # label is O\n",
    "                    y_.append(0)        \n",
    "            y_te_true.append(torch.Tensor(y_))\n",
    "        y_te_true = torch.stack(y_te_true)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['agak', 'perlu', 'dibersihkan', '.', 'kamar', 'banyak', 'semut', '.']\n",
      "{('5-5', '1-3'): 'NG', ('5-5', '6-7'): 'NG'}\n",
      "tensor([0, 0, 0,  ..., 0, 0, 0], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "IDX = 10\n",
    "\n",
    "print(sentences[IDX])\n",
    "print(current_relation_dict[IDX])\n",
    "y_pred = logits_relation_scorer.argmax(-1)\n",
    "print(y_pred[IDX])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pelayanan', 'lumayan', 'baik', '.']\n",
      "{'1-1': 'ASPECT', '2-3': 'SENTIMENT'}\n",
      "tensor([0, 1, 2, 0, 2, 0, 2, 0, 2, 0, 0, 1, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0], device='cuda:0')\n",
      "tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "IDX = 6\n",
    "\n",
    "print(sentences[IDX])\n",
    "print(current_te_label_dict[IDX])\n",
    "y_pred = logits_term_scorer.argmax(-1)\n",
    "print(y_pred[IDX])\n",
    "print(y_te_true[IDX])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1   1   1-1\n",
      "2   2   2-2\n",
      "4   2   4-4\n",
      "6   2   6-6\n",
      "8   2   8-8\n",
      "11   1   11-11\n",
      "12   2   12-12\n",
      "17   2   17-17\n",
      "56   2   16-17\n"
     ]
    }
   ],
   "source": [
    "for idx, y in enumerate(y_pred[IDX]):\n",
    "    if y != 0:\n",
    "        span_range = span_ranges[idx]\n",
    "        print(idx, ' ', y.item(), ' ', span_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.', 'kamar']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[10][3:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
