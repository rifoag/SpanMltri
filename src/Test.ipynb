{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "from BaseEncoder import BaseEncoder\n",
    "from DataLoader import read_examples_from_file, ReviewDataset\n",
    "\n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from SpanMltri import SpanMltri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Base Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'Aku seorang kapiten'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_encoder = BaseEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_tokenized = base_encoder.tokenize(['Aku', 'seorang', 'kapiten'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([40])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_tokenized.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = \"dataset/test_2k_output_Feb_26_2020_output_Feb_18_2021.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = ReviewDataset(FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "span_mltri = SpanMltri()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "otw backprop\n",
      "done backprop\n",
      "loss: 0.876627  [    0/  200]\n",
      "otw backprop\n",
      "done backprop\n",
      "otw backprop\n",
      "done backprop\n",
      "loss: 1.075147  [   16/  200]\n",
      "otw backprop\n",
      "done backprop\n",
      "otw backprop\n",
      "done backprop\n",
      "loss: 1.073519  [   32/  200]\n",
      "otw backprop\n",
      "done backprop\n",
      "otw backprop\n",
      "done backprop\n",
      "loss: 0.946236  [   48/  200]\n",
      "otw backprop\n",
      "done backprop\n",
      "otw backprop\n",
      "done backprop\n",
      "loss: 0.946233  [   64/  200]\n",
      "otw backprop\n",
      "done backprop\n",
      "otw backprop\n",
      "done backprop\n",
      "loss: 0.946175  [   80/  200]\n",
      "otw backprop\n",
      "done backprop\n",
      "otw backprop\n",
      "done backprop\n",
      "loss: 0.946003  [   96/  200]\n",
      "otw backprop\n",
      "done backprop\n",
      "otw backprop\n",
      "done backprop\n",
      "loss: 0.946404  [  112/  200]\n",
      "otw backprop\n",
      "done backprop\n",
      "otw backprop\n",
      "done backprop\n",
      "loss: 0.946118  [  128/  200]\n",
      "otw backprop\n",
      "done backprop\n",
      "otw backprop\n",
      "done backprop\n",
      "loss: 0.945775  [  144/  200]\n",
      "otw backprop\n",
      "done backprop\n",
      "otw backprop\n",
      "done backprop\n",
      "loss: 0.946061  [  160/  200]\n",
      "otw backprop\n",
      "done backprop\n",
      "otw backprop\n",
      "done backprop\n",
      "loss: 0.945832  [  176/  200]\n",
      "otw backprop\n",
      "done backprop\n",
      "otw backprop\n",
      "done backprop\n",
      "loss: 0.946175  [  192/  200]\n",
      "Wall time: 2min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "BATCH_SIZE = 8\n",
    "test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(span_mltri.parameters(), lr=0.005)\n",
    "\n",
    "lambda_t = 0.5\n",
    "lambda_r = 0.5\n",
    "\n",
    "size = len(test_dataloader.dataset)\n",
    "for batch, X in enumerate(test_dataloader):\n",
    "    current_te_label_list = test_data.te_label_list[(batch)*BATCH_SIZE:(batch+1)*BATCH_SIZE]\n",
    "    current_relations = test_data.relations[(batch)*BATCH_SIZE:(batch+1)*BATCH_SIZE]\n",
    "    \n",
    "    logits_term_scorer, span_ranges, logits_relation_scorer, span_pair_ranges = span_mltri(X)\n",
    "    \n",
    "    y_te_true = []\n",
    "    CURRENT_BATCH_SIZE = min(len(current_te_label_list), BATCH_SIZE)\n",
    "    for i in range(CURRENT_BATCH_SIZE):\n",
    "        y_ = []\n",
    "        for span_range in span_ranges:\n",
    "            if span_range in current_te_label_list[i]:\n",
    "                label = current_te_label_list[i][span_range]\n",
    "                if label == 'ASPECT':\n",
    "                    y_.append(1)\n",
    "                elif label == 'SENTIMENT':\n",
    "                    y_.append(2)\n",
    "            else: # label is O\n",
    "                y_.append(0)        \n",
    "        y_te_true.append(torch.Tensor(y_))\n",
    "    y_te_true = torch.stack(y_te_true)\n",
    "    y_te_true = y_te_true.to(torch.long)\n",
    "    \n",
    "    logits_term_scorer = logits_term_scorer.reshape(logits_term_scorer.shape[0]*logits_term_scorer.shape[1], logits_term_scorer.shape[-1])\n",
    "    y_te_true = y_te_true.reshape(-1)\n",
    "    te_loss = loss_fn(logits_term_scorer, y_te_true)\n",
    "    \n",
    "    y_paote_true = []\n",
    "    CURRENT_BATCH_SIZE = min(len(current_relations), BATCH_SIZE)\n",
    "    for i in range(CURRENT_BATCH_SIZE):\n",
    "        y_ = []\n",
    "        for span_pair_range in span_pair_ranges[i]:\n",
    "            if span_pair_range not in current_relations[i]:\n",
    "                y_.append(0)\n",
    "            else:\n",
    "                label = current_relations[i][span_pair_range]\n",
    "                if label == 'PO':\n",
    "                    y_.append(1)\n",
    "                elif label == 'NG':\n",
    "                    y_.append(2)\n",
    "                elif label == 'NT':\n",
    "                    y_.append(3)\n",
    "        y_paote_true.append(torch.Tensor(y_))\n",
    "    y_paote_true = torch.stack(y_paote_true)\n",
    "    y_paote_true = y_paote_true.to(torch.long)\n",
    "    \n",
    "    logits_relation_scorer = logits_relation_scorer.reshape(logits_relation_scorer.shape[0]*logits_relation_scorer.shape[1], logits_relation_scorer.shape[-1])\n",
    "    y_paote_true = y_paote_true.reshape(-1)\n",
    "    paote_loss = loss_fn(logits_relation_scorer, y_paote_true)\n",
    "    \n",
    "    total_loss = lambda_t*te_loss + lambda_r*paote_loss\n",
    "    \n",
    "    # Backpropagation\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if batch % 2 == 0:\n",
    "        total_loss, current = total_loss.item(), batch * len(X)\n",
    "        print(f\"loss: {total_loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure:  SpanMltri(\n",
      "  (base_encoder): BaseEncoder(\n",
      "    (model): BertModel(\n",
      "      (embeddings): BertEmbeddings(\n",
      "        (word_embeddings): Embedding(31923, 768, padding_idx=0)\n",
      "        (position_embeddings): Embedding(512, 768)\n",
      "        (token_type_embeddings): Embedding(2, 768)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (encoder): BertEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (6): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (7): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (8): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (9): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (10): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (11): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pooler): BertPooler(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (activation): Tanh()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (term_scorer): TermScorer(\n",
      "    (linear_softmax): Sequential(\n",
      "      (0): Linear(in_features=768, out_features=3, bias=True)\n",
      "      (1): Sigmoid()\n",
      "      (2): Softmax(dim=-1)\n",
      "    )\n",
      "  )\n",
      "  (relation_scorer): RelationScorer(\n",
      "    (span_scorer): Sequential(\n",
      "      (0): Linear(in_features=768, out_features=3, bias=True)\n",
      "      (1): Sigmoid()\n",
      "    )\n",
      "    (pair_scorer): Sequential(\n",
      "      (0): Linear(in_features=2304, out_features=3, bias=True)\n",
      "      (1): Sigmoid()\n",
      "    )\n",
      "    (final_softmax): Softmax(dim=-1)\n",
      "  )\n",
      ") \n",
      "\n",
      "\n",
      "Layer: base_encoder.model.embeddings.word_embeddings.weight | Size: torch.Size([31923, 768]) | Values : tensor([[-0.0584, -0.0798,  0.1704,  ..., -0.1105, -0.0408, -0.0498],\n",
      "        [-0.0128,  0.0402,  0.2200,  ..., -0.0414,  0.0109, -0.0024]],\n",
      "       grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.embeddings.position_embeddings.weight | Size: torch.Size([512, 768]) | Values : tensor([[ 0.0932,  0.0018, -0.0455,  ..., -0.0893, -0.0125, -0.0142],\n",
      "        [ 0.0278,  0.0136,  0.0030,  ...,  0.0303, -0.0215, -0.0244]],\n",
      "       grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.embeddings.token_type_embeddings.weight | Size: torch.Size([2, 768]) | Values : tensor([[ 0.0098,  0.0403, -0.0079,  ..., -0.0071,  0.0030, -0.0299],\n",
      "        [ 0.0302, -0.0382, -0.0130,  ..., -0.0153, -0.0045, -0.0077]],\n",
      "       grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.embeddings.LayerNorm.weight | Size: torch.Size([768]) | Values : tensor([0.7756, 0.8292], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.embeddings.LayerNorm.bias | Size: torch.Size([768]) | Values : tensor([-0.0806,  0.0410], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.0.attention.self.query.weight | Size: torch.Size([768, 768]) | Values : tensor([[-0.0063,  0.0358,  0.0557,  ..., -0.0455, -0.0097, -0.0146],\n",
      "        [ 0.0435,  0.0514,  0.0212,  ...,  0.0351,  0.0075, -0.0233]],\n",
      "       grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.0.attention.self.query.bias | Size: torch.Size([768]) | Values : tensor([ 0.0070, -0.0594], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.0.attention.self.key.weight | Size: torch.Size([768, 768]) | Values : tensor([[-0.0332, -0.0010,  0.0555,  ..., -0.0138, -0.0087, -0.0352],\n",
      "        [ 0.0949, -0.0198,  0.0355,  ..., -0.0681,  0.0197,  0.1451]],\n",
      "       grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.0.attention.self.key.bias | Size: torch.Size([768]) | Values : tensor([-0.0002, -0.0002], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.0.attention.self.value.weight | Size: torch.Size([768, 768]) | Values : tensor([[ 2.9872e-02, -4.8742e-02,  1.8776e-02,  ...,  4.1315e-05,\n",
      "         -3.3311e-02, -6.8248e-02],\n",
      "        [ 8.6752e-03,  5.0709e-03, -3.9376e-02,  ..., -2.5127e-02,\n",
      "         -1.7415e-02, -1.0140e-01]], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.0.attention.self.value.bias | Size: torch.Size([768]) | Values : tensor([0.0146, 0.0118], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.0.attention.output.dense.weight | Size: torch.Size([768, 768]) | Values : tensor([[-0.0280,  0.0533,  0.0272,  ..., -0.0158,  0.0176,  0.0473],\n",
      "        [-0.0233, -0.0213, -0.0385,  ...,  0.0014, -0.0566,  0.0401]],\n",
      "       grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.0.attention.output.dense.bias | Size: torch.Size([768]) | Values : tensor([0.0218, 0.0124], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.0.attention.output.LayerNorm.weight | Size: torch.Size([768]) | Values : tensor([1.0222, 0.9796], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.0.attention.output.LayerNorm.bias | Size: torch.Size([768]) | Values : tensor([0.5972, 0.1118], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.0.intermediate.dense.weight | Size: torch.Size([3072, 768]) | Values : tensor([[-0.0360,  0.0059, -0.0332,  ...,  0.0219,  0.0141,  0.0478],\n",
      "        [-0.0259, -0.0217, -0.0260,  ..., -0.0152, -0.0508,  0.0287]],\n",
      "       grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.0.intermediate.dense.bias | Size: torch.Size([3072]) | Values : tensor([-0.0646, -0.0521], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.0.output.dense.weight | Size: torch.Size([768, 3072]) | Values : tensor([[-0.0217, -0.0182,  0.0962,  ..., -0.0432,  0.0550, -0.0314],\n",
      "        [ 0.0125,  0.0206,  0.0384,  ..., -0.0354, -0.0125, -0.0108]],\n",
      "       grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.0.output.dense.bias | Size: torch.Size([768]) | Values : tensor([-0.0005,  0.0098], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.0.output.LayerNorm.weight | Size: torch.Size([768]) | Values : tensor([0.6892, 0.7435], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.0.output.LayerNorm.bias | Size: torch.Size([768]) | Values : tensor([-0.2122, -0.0382], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.1.attention.self.query.weight | Size: torch.Size([768, 768]) | Values : tensor([[ 0.0374, -0.0627, -0.0444,  ...,  0.0168,  0.0691,  0.0665],\n",
      "        [ 0.0551,  0.0485, -0.1099,  ..., -0.0253,  0.0210,  0.0208]],\n",
      "       grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.1.attention.self.query.bias | Size: torch.Size([768]) | Values : tensor([-0.0092, -0.0592], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.1.attention.self.key.weight | Size: torch.Size([768, 768]) | Values : tensor([[ 0.0250, -0.0357,  0.0537,  ...,  0.0164,  0.0631, -0.0175],\n",
      "        [-0.0170,  0.0085,  0.1140,  ..., -0.0335,  0.0317,  0.0235]],\n",
      "       grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.1.attention.self.key.bias | Size: torch.Size([768]) | Values : tensor([ 1.7763e-06, -2.1573e-04], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.1.attention.self.value.weight | Size: torch.Size([768, 768]) | Values : tensor([[ 0.0043, -0.0154,  0.0320,  ..., -0.0036, -0.0240,  0.0278],\n",
      "        [-0.0212, -0.0737, -0.0489,  ..., -0.0161, -0.0035, -0.0127]],\n",
      "       grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.1.attention.self.value.bias | Size: torch.Size([768]) | Values : tensor([0.0509, 0.0388], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.1.attention.output.dense.weight | Size: torch.Size([768, 768]) | Values : tensor([[ 0.0144,  0.0312, -0.0053,  ..., -0.0188, -0.0129,  0.0062],\n",
      "        [-0.0090, -0.0714,  0.0116,  ..., -0.0060,  0.0046,  0.0431]],\n",
      "       grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.1.attention.output.dense.bias | Size: torch.Size([768]) | Values : tensor([-0.0334, -0.0141], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.1.attention.output.LayerNorm.weight | Size: torch.Size([768]) | Values : tensor([0.9561, 0.8819], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.1.attention.output.LayerNorm.bias | Size: torch.Size([768]) | Values : tensor([0.4358, 0.0219], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.1.intermediate.dense.weight | Size: torch.Size([3072, 768]) | Values : tensor([[-0.0489,  0.0858, -0.1106,  ...,  0.0272,  0.0439, -0.0914],\n",
      "        [-0.0657,  0.0501, -0.1458,  ...,  0.0620, -0.0122, -0.0160]],\n",
      "       grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.1.intermediate.dense.bias | Size: torch.Size([3072]) | Values : tensor([-0.0599, -0.1132], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.1.output.dense.weight | Size: torch.Size([768, 3072]) | Values : tensor([[ 0.0091,  0.0078,  0.0719,  ..., -0.0683,  0.0811, -0.0058],\n",
      "        [ 0.0198,  0.0231, -0.0552,  ...,  0.0261, -0.0074, -0.0339]],\n",
      "       grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.1.output.dense.bias | Size: torch.Size([768]) | Values : tensor([ 0.0683, -0.0015], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.1.output.LayerNorm.weight | Size: torch.Size([768]) | Values : tensor([0.7526, 0.8508], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.1.output.LayerNorm.bias | Size: torch.Size([768]) | Values : tensor([-0.2843, -0.0514], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.2.attention.self.query.weight | Size: torch.Size([768, 768]) | Values : tensor([[ 0.0090,  0.0024,  0.0457,  ..., -0.0986, -0.0208, -0.0014],\n",
      "        [-0.0812,  0.0531,  0.0159,  ...,  0.0423, -0.0452, -0.0413]],\n",
      "       grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.2.attention.self.query.bias | Size: torch.Size([768]) | Values : tensor([-0.3017,  0.3362], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.2.attention.self.key.weight | Size: torch.Size([768, 768]) | Values : tensor([[ 0.0011,  0.0448,  0.0258,  ...,  0.0594, -0.0186,  0.0017],\n",
      "        [-0.0101,  0.0491, -0.0536,  ..., -0.0265,  0.0435,  0.0217]],\n",
      "       grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.2.attention.self.key.bias | Size: torch.Size([768]) | Values : tensor([1.4556e-04, 5.3287e-05], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.2.attention.self.value.weight | Size: torch.Size([768, 768]) | Values : tensor([[ 0.0854, -0.0340, -0.0023,  ...,  0.0532, -0.0698, -0.0184],\n",
      "        [-0.0105,  0.0222,  0.0213,  ...,  0.0753,  0.0157, -0.0614]],\n",
      "       grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.2.attention.self.value.bias | Size: torch.Size([768]) | Values : tensor([0.0185, 0.0062], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.2.attention.output.dense.weight | Size: torch.Size([768, 768]) | Values : tensor([[ 0.0521,  0.0907,  0.0496,  ..., -0.0034, -0.0108,  0.0072],\n",
      "        [-0.0020, -0.0803,  0.0057,  ...,  0.0268, -0.0116, -0.0203]],\n",
      "       grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.2.attention.output.dense.bias | Size: torch.Size([768]) | Values : tensor([-0.0084,  0.0543], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.2.attention.output.LayerNorm.weight | Size: torch.Size([768]) | Values : tensor([0.8931, 0.8119], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.2.attention.output.LayerNorm.bias | Size: torch.Size([768]) | Values : tensor([ 0.3135, -0.0109], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.2.intermediate.dense.weight | Size: torch.Size([3072, 768]) | Values : tensor([[-0.0470,  0.0056, -0.0443,  ..., -0.0057, -0.0021, -0.0076],\n",
      "        [-0.0781, -0.0149, -0.0350,  ..., -0.0202, -0.0571, -0.0568]],\n",
      "       grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.2.intermediate.dense.bias | Size: torch.Size([3072]) | Values : tensor([-0.0766, -0.0601], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.2.output.dense.weight | Size: torch.Size([768, 3072]) | Values : tensor([[ 0.0647, -0.0414, -0.0726,  ..., -0.0307,  0.0092, -0.0533],\n",
      "        [ 0.0757,  0.0496, -0.0067,  ..., -0.0140, -0.0347,  0.0162]],\n",
      "       grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.2.output.dense.bias | Size: torch.Size([768]) | Values : tensor([-0.0218,  0.0395], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.2.output.LayerNorm.weight | Size: torch.Size([768]) | Values : tensor([0.7843, 0.7782], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.2.output.LayerNorm.bias | Size: torch.Size([768]) | Values : tensor([-0.2255,  0.0438], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.3.attention.self.query.weight | Size: torch.Size([768, 768]) | Values : tensor([[-0.0476, -0.0007,  0.0579,  ..., -0.0530, -0.0006, -0.0372],\n",
      "        [-0.0158,  0.1089,  0.0062,  ...,  0.0480, -0.0175,  0.0845]],\n",
      "       grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.3.attention.self.query.bias | Size: torch.Size([768]) | Values : tensor([-0.2703, -0.0966], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.3.attention.self.key.weight | Size: torch.Size([768, 768]) | Values : tensor([[ 0.0152,  0.0440,  0.0862,  ..., -0.0022,  0.0466, -0.0150],\n",
      "        [ 0.0425, -0.0394,  0.0668,  ..., -0.1090, -0.0026, -0.0303]],\n",
      "       grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.3.attention.self.key.bias | Size: torch.Size([768]) | Values : tensor([-8.9814e-05, -1.2345e-04], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.3.attention.self.value.weight | Size: torch.Size([768, 768]) | Values : tensor([[-0.0676, -0.0482, -0.0360,  ..., -0.0091, -0.0189,  0.0217],\n",
      "        [-0.0221, -0.0255, -0.0680,  ...,  0.0248, -0.0097,  0.0317]],\n",
      "       grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.3.attention.self.value.bias | Size: torch.Size([768]) | Values : tensor([0.0139, 0.0373], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.3.attention.output.dense.weight | Size: torch.Size([768, 768]) | Values : tensor([[ 0.0466, -0.0452, -0.1113,  ...,  0.0443,  0.0290, -0.0203],\n",
      "        [-0.0385, -0.0414,  0.0320,  ..., -0.0175, -0.0536,  0.0240]],\n",
      "       grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.3.attention.output.dense.bias | Size: torch.Size([768]) | Values : tensor([-0.0627,  0.0575], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.3.attention.output.LayerNorm.weight | Size: torch.Size([768]) | Values : tensor([0.8361, 0.8163], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.3.attention.output.LayerNorm.bias | Size: torch.Size([768]) | Values : tensor([0.1775, 0.0073], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.3.intermediate.dense.weight | Size: torch.Size([3072, 768]) | Values : tensor([[ 0.0651, -0.0627, -0.0045,  ...,  0.0273,  0.0209, -0.0398],\n",
      "        [ 0.0091, -0.0127,  0.0303,  ...,  0.0242,  0.0387,  0.0392]],\n",
      "       grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.3.intermediate.dense.bias | Size: torch.Size([3072]) | Values : tensor([-0.0619, -0.0474], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.3.output.dense.weight | Size: torch.Size([768, 3072]) | Values : tensor([[-0.0312, -0.0887, -0.0308,  ...,  0.0236, -0.0505, -0.0166],\n",
      "        [-0.0084, -0.0701,  0.0505,  ...,  0.0267,  0.0281,  0.0556]],\n",
      "       grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.3.output.dense.bias | Size: torch.Size([768]) | Values : tensor([-0.0086,  0.0944], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.3.output.LayerNorm.weight | Size: torch.Size([768]) | Values : tensor([0.8526, 0.8502], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.3.output.LayerNorm.bias | Size: torch.Size([768]) | Values : tensor([-0.1761,  0.0196], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.4.attention.self.query.weight | Size: torch.Size([768, 768]) | Values : tensor([[ 0.0056, -0.1280,  0.0388,  ..., -0.1006, -0.1765,  0.0256],\n",
      "        [-0.0423,  0.1352,  0.1804,  ..., -0.1963,  0.1661, -0.0180]],\n",
      "       grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.4.attention.self.query.bias | Size: torch.Size([768]) | Values : tensor([0.0025, 0.1972], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.4.attention.self.key.weight | Size: torch.Size([768, 768]) | Values : tensor([[ 0.0580, -0.0085,  0.0079,  ..., -0.0956, -0.0468, -0.0322],\n",
      "        [-0.0112,  0.0245,  0.0377,  ..., -0.0107,  0.2058,  0.0402]],\n",
      "       grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.4.attention.self.key.bias | Size: torch.Size([768]) | Values : tensor([-0.0008, -0.0067], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.4.attention.self.value.weight | Size: torch.Size([768, 768]) | Values : tensor([[ 0.0363,  0.0193,  0.0224,  ...,  0.0107,  0.0049, -0.0164],\n",
      "        [-0.0658, -0.0209,  0.0148,  ..., -0.0221, -0.0437,  0.0070]],\n",
      "       grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.4.attention.self.value.bias | Size: torch.Size([768]) | Values : tensor([0.0356, 0.0796], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.4.attention.output.dense.weight | Size: torch.Size([768, 768]) | Values : tensor([[-0.0345, -0.0370,  0.0550,  ...,  0.0012,  0.0515, -0.0070],\n",
      "        [ 0.0152,  0.0300, -0.0490,  ..., -0.0459,  0.0071,  0.0207]],\n",
      "       grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.4.attention.output.dense.bias | Size: torch.Size([768]) | Values : tensor([-0.0243,  0.0448], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.4.attention.output.LayerNorm.weight | Size: torch.Size([768]) | Values : tensor([0.8601, 0.8606], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.4.attention.output.LayerNorm.bias | Size: torch.Size([768]) | Values : tensor([ 0.0777, -0.0122], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.4.intermediate.dense.weight | Size: torch.Size([3072, 768]) | Values : tensor([[ 4.0629e-03, -6.4293e-02,  3.0709e-02,  ...,  7.6727e-02,\n",
      "          2.0370e-02,  7.2915e-05],\n",
      "        [ 1.6482e-02, -6.4708e-02,  3.7216e-02,  ..., -4.2735e-03,\n",
      "          2.0909e-02,  6.1455e-02]], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.4.intermediate.dense.bias | Size: torch.Size([3072]) | Values : tensor([-0.0620, -0.0875], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.4.output.dense.weight | Size: torch.Size([768, 3072]) | Values : tensor([[-0.0416, -0.0758, -0.0149,  ...,  0.0958,  0.0208, -0.0228],\n",
      "        [-0.0291, -0.0456, -0.0472,  ...,  0.0130,  0.0396, -0.0383]],\n",
      "       grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.4.output.dense.bias | Size: torch.Size([768]) | Values : tensor([0.0649, 0.0885], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.4.output.LayerNorm.weight | Size: torch.Size([768]) | Values : tensor([0.8240, 0.8219], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.4.output.LayerNorm.bias | Size: torch.Size([768]) | Values : tensor([-0.1220, -0.0417], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.5.attention.self.query.weight | Size: torch.Size([768, 768]) | Values : tensor([[ 0.0271,  0.0295, -0.0077,  ...,  0.0988, -0.0579,  0.0039],\n",
      "        [ 0.2010,  0.0361,  0.0364,  ...,  0.0555,  0.0717,  0.0487]],\n",
      "       grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.5.attention.self.query.bias | Size: torch.Size([768]) | Values : tensor([ 0.1122, -0.0162], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.5.attention.self.key.weight | Size: torch.Size([768, 768]) | Values : tensor([[ 0.0389,  0.0349, -0.0336,  ...,  0.0540, -0.0375,  0.0580],\n",
      "        [ 0.0232, -0.0612,  0.0299,  ...,  0.0560,  0.0226, -0.0653]],\n",
      "       grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.5.attention.self.key.bias | Size: torch.Size([768]) | Values : tensor([4.4991e-05, 7.0372e-05], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.5.attention.self.value.weight | Size: torch.Size([768, 768]) | Values : tensor([[ 0.0692,  0.0617, -0.0468,  ..., -0.0153,  0.0085, -0.1424],\n",
      "        [-0.0110,  0.0512,  0.0132,  ...,  0.0116, -0.0398,  0.0428]],\n",
      "       grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.5.attention.self.value.bias | Size: torch.Size([768]) | Values : tensor([0.0003, 0.0153], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.5.attention.output.dense.weight | Size: torch.Size([768, 768]) | Values : tensor([[-0.0032, -0.0328,  0.0018,  ..., -0.0653,  0.0572,  0.0358],\n",
      "        [-0.0275, -0.1000,  0.0096,  ...,  0.0020,  0.0480,  0.0206]],\n",
      "       grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.5.attention.output.dense.bias | Size: torch.Size([768]) | Values : tensor([-0.2206, -0.0169], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.5.attention.output.LayerNorm.weight | Size: torch.Size([768]) | Values : tensor([0.8108, 0.7770], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.5.attention.output.LayerNorm.bias | Size: torch.Size([768]) | Values : tensor([ 0.1588, -0.1326], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.5.intermediate.dense.weight | Size: torch.Size([3072, 768]) | Values : tensor([[-0.0566, -0.0087,  0.1077,  ..., -0.0277, -0.0052,  0.0237],\n",
      "        [ 0.0205, -0.0484, -0.1096,  ..., -0.0317, -0.0126,  0.0022]],\n",
      "       grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.5.intermediate.dense.bias | Size: torch.Size([3072]) | Values : tensor([-0.0883, -0.0760], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.5.output.dense.weight | Size: torch.Size([768, 3072]) | Values : tensor([[-0.0482,  0.0704,  0.0016,  ...,  0.0373,  0.0303,  0.0230],\n",
      "        [ 0.0299,  0.0220, -0.1716,  ..., -0.0815,  0.0314, -0.0576]],\n",
      "       grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.5.output.dense.bias | Size: torch.Size([768]) | Values : tensor([ 0.0816, -0.0110], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.5.output.LayerNorm.weight | Size: torch.Size([768]) | Values : tensor([0.7615, 0.8252], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.5.output.LayerNorm.bias | Size: torch.Size([768]) | Values : tensor([-0.1445, -0.0194], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.6.attention.self.query.weight | Size: torch.Size([768, 768]) | Values : tensor([[ 0.0664,  0.1143,  0.0233,  ..., -0.0175, -0.0580,  0.0828],\n",
      "        [ 0.1644,  0.0795,  0.1027,  ..., -0.0029, -0.0388,  0.0331]],\n",
      "       grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.6.attention.self.query.bias | Size: torch.Size([768]) | Values : tensor([-0.0713,  0.0803], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.6.attention.self.key.weight | Size: torch.Size([768, 768]) | Values : tensor([[ 0.0732,  0.0793, -0.0119,  ...,  0.0743,  0.1225,  0.0422],\n",
      "        [ 0.0113, -0.0555, -0.0107,  ..., -0.0004, -0.0694,  0.0229]],\n",
      "       grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.6.attention.self.key.bias | Size: torch.Size([768]) | Values : tensor([ 0.0001, -0.0001], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.6.attention.self.value.weight | Size: torch.Size([768, 768]) | Values : tensor([[ 0.0043,  0.1194, -0.0228,  ...,  0.0271,  0.0494, -0.0163],\n",
      "        [-0.0243,  0.0880,  0.0658,  ...,  0.0040,  0.0045,  0.0305]],\n",
      "       grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.6.attention.self.value.bias | Size: torch.Size([768]) | Values : tensor([-0.0212, -0.0344], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.6.attention.output.dense.weight | Size: torch.Size([768, 768]) | Values : tensor([[-0.0311,  0.0301, -0.0183,  ..., -0.0270,  0.0220,  0.0317],\n",
      "        [-0.0566,  0.0541, -0.0555,  ...,  0.1080,  0.0122,  0.0238]],\n",
      "       grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.6.attention.output.dense.bias | Size: torch.Size([768]) | Values : tensor([-0.1536,  0.0507], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.6.attention.output.LayerNorm.weight | Size: torch.Size([768]) | Values : tensor([0.8356, 0.8024], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.6.attention.output.LayerNorm.bias | Size: torch.Size([768]) | Values : tensor([ 0.2131, -0.0876], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.6.intermediate.dense.weight | Size: torch.Size([3072, 768]) | Values : tensor([[ 0.0278,  0.0130, -0.0252,  ...,  0.0284,  0.0387,  0.0444],\n",
      "        [ 0.0303, -0.0337,  0.0129,  ...,  0.0681,  0.0249, -0.0586]],\n",
      "       grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.6.intermediate.dense.bias | Size: torch.Size([3072]) | Values : tensor([-0.0317, -0.0047], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.6.output.dense.weight | Size: torch.Size([768, 3072]) | Values : tensor([[-0.0309, -0.0348, -0.0519,  ...,  0.0489,  0.0157,  0.0244],\n",
      "        [ 0.0299,  0.0093,  0.0206,  ...,  0.0904,  0.0125, -0.0194]],\n",
      "       grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.6.output.dense.bias | Size: torch.Size([768]) | Values : tensor([ 0.0150, -0.0178], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.6.output.LayerNorm.weight | Size: torch.Size([768]) | Values : tensor([0.8175, 0.8425], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.6.output.LayerNorm.bias | Size: torch.Size([768]) | Values : tensor([-0.1513,  0.0220], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.7.attention.self.query.weight | Size: torch.Size([768, 768]) | Values : tensor([[-0.0044,  0.0931,  0.0448,  ...,  0.0686,  0.0220, -0.0666],\n",
      "        [-0.0508,  0.0096, -0.0301,  ...,  0.0809,  0.0328, -0.0334]],\n",
      "       grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.7.attention.self.query.bias | Size: torch.Size([768]) | Values : tensor([-0.0579,  0.0939], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.7.attention.self.key.weight | Size: torch.Size([768, 768]) | Values : tensor([[ 0.0148, -0.0370, -0.2157,  ..., -0.0506, -0.0168, -0.0302],\n",
      "        [-0.0338,  0.0802,  0.0096,  ...,  0.0497, -0.1280,  0.0093]],\n",
      "       grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.7.attention.self.key.bias | Size: torch.Size([768]) | Values : tensor([-0.0092,  0.0073], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.7.attention.self.value.weight | Size: torch.Size([768, 768]) | Values : tensor([[-0.0929, -0.0455,  0.0284,  ...,  0.0152,  0.0002,  0.0098],\n",
      "        [-0.0598, -0.0138,  0.0323,  ..., -0.0156, -0.0547,  0.0045]],\n",
      "       grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.7.attention.self.value.bias | Size: torch.Size([768]) | Values : tensor([-0.0022, -0.0477], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.7.attention.output.dense.weight | Size: torch.Size([768, 768]) | Values : tensor([[-0.0128, -0.0931,  0.0023,  ...,  0.0466,  0.0806, -0.0729],\n",
      "        [-0.0463, -0.0033,  0.0434,  ...,  0.0322,  0.0152, -0.0194]],\n",
      "       grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.7.attention.output.dense.bias | Size: torch.Size([768]) | Values : tensor([-0.0578, -0.0099], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.7.attention.output.LayerNorm.weight | Size: torch.Size([768]) | Values : tensor([0.7742, 0.8056], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.7.attention.output.LayerNorm.bias | Size: torch.Size([768]) | Values : tensor([ 0.0392, -0.1218], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.7.intermediate.dense.weight | Size: torch.Size([3072, 768]) | Values : tensor([[ 0.0406, -0.0439, -0.0090,  ...,  0.0023,  0.0316, -0.0693],\n",
      "        [-0.0507,  0.0107,  0.1605,  ..., -0.0214, -0.0317,  0.0962]],\n",
      "       grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.7.intermediate.dense.bias | Size: torch.Size([3072]) | Values : tensor([-0.0549, -0.0148], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.7.output.dense.weight | Size: torch.Size([768, 3072]) | Values : tensor([[ 0.0345,  0.0129,  0.0305,  ...,  0.0021, -0.0102, -0.0793],\n",
      "        [ 0.0583,  0.0281,  0.0568,  ...,  0.0649,  0.0870,  0.0203]],\n",
      "       grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.7.output.dense.bias | Size: torch.Size([768]) | Values : tensor([ 0.0212, -0.0232], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.7.output.LayerNorm.weight | Size: torch.Size([768]) | Values : tensor([0.7918, 0.8625], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.7.output.LayerNorm.bias | Size: torch.Size([768]) | Values : tensor([-0.0694, -0.0190], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.8.attention.self.query.weight | Size: torch.Size([768, 768]) | Values : tensor([[-0.0413, -0.0653,  0.0275,  ...,  0.0594,  0.0211, -0.0371],\n",
      "        [ 0.1169,  0.0569,  0.0843,  ...,  0.0540,  0.0425,  0.1008]],\n",
      "       grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.8.attention.self.query.bias | Size: torch.Size([768]) | Values : tensor([ 0.0494, -0.0427], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.8.attention.self.key.weight | Size: torch.Size([768, 768]) | Values : tensor([[-0.0124, -0.0444,  0.0555,  ..., -0.0496, -0.0543,  0.0267],\n",
      "        [-0.1036,  0.0495, -0.0837,  ...,  0.0468,  0.0434,  0.0087]],\n",
      "       grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.8.attention.self.key.bias | Size: torch.Size([768]) | Values : tensor([0.0026, 0.0029], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.8.attention.self.value.weight | Size: torch.Size([768, 768]) | Values : tensor([[-5.9867e-02,  1.7924e-02,  2.9691e-02,  ...,  6.4248e-02,\n",
      "         -1.5682e-02, -2.7889e-02],\n",
      "        [-6.4364e-05, -3.3482e-02, -1.9980e-02,  ..., -3.5459e-02,\n",
      "         -1.7754e-02, -1.9204e-02]], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.8.attention.self.value.bias | Size: torch.Size([768]) | Values : tensor([-0.0439, -0.0164], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.8.attention.output.dense.weight | Size: torch.Size([768, 768]) | Values : tensor([[-0.0425,  0.0120,  0.0177,  ...,  0.0123, -0.0427,  0.0257],\n",
      "        [-0.0485, -0.0077,  0.0312,  ..., -0.0142, -0.0057, -0.0578]],\n",
      "       grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.8.attention.output.dense.bias | Size: torch.Size([768]) | Values : tensor([-0.0371, -0.0409], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.8.attention.output.LayerNorm.weight | Size: torch.Size([768]) | Values : tensor([0.7781, 0.8682], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.8.attention.output.LayerNorm.bias | Size: torch.Size([768]) | Values : tensor([ 0.0503, -0.0584], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.8.intermediate.dense.weight | Size: torch.Size([3072, 768]) | Values : tensor([[ 0.0224,  0.0797, -0.0258,  ..., -0.0789,  0.0830, -0.0044],\n",
      "        [-0.0444, -0.0708, -0.0170,  ..., -0.0565,  0.0460, -0.0544]],\n",
      "       grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.8.intermediate.dense.bias | Size: torch.Size([3072]) | Values : tensor([-0.0849, -0.0123], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.8.output.dense.weight | Size: torch.Size([768, 3072]) | Values : tensor([[ 0.0752, -0.0319, -0.0412,  ..., -0.0088,  0.0374, -0.0702],\n",
      "        [ 0.0256,  0.0510,  0.0343,  ..., -0.0337,  0.0090,  0.0078]],\n",
      "       grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.8.output.dense.bias | Size: torch.Size([768]) | Values : tensor([0.0095, 0.0236], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.8.output.LayerNorm.weight | Size: torch.Size([768]) | Values : tensor([0.7387, 0.7302], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.8.output.LayerNorm.bias | Size: torch.Size([768]) | Values : tensor([-0.0694,  0.0049], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.9.attention.self.query.weight | Size: torch.Size([768, 768]) | Values : tensor([[ 0.0254, -0.0813,  0.0025,  ..., -0.0016, -0.0523,  0.1018],\n",
      "        [ 0.0328, -0.0034,  0.1230,  ..., -0.0230, -0.0573, -0.0552]],\n",
      "       grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.9.attention.self.query.bias | Size: torch.Size([768]) | Values : tensor([-0.0257,  0.0208], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.9.attention.self.key.weight | Size: torch.Size([768, 768]) | Values : tensor([[-0.0793, -0.1108,  0.0167,  ...,  0.0775, -0.0031,  0.1245],\n",
      "        [ 0.0410,  0.0015,  0.0714,  ..., -0.0689, -0.0003, -0.1304]],\n",
      "       grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.9.attention.self.key.bias | Size: torch.Size([768]) | Values : tensor([-0.0139,  0.0159], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.9.attention.self.value.weight | Size: torch.Size([768, 768]) | Values : tensor([[-0.0389, -0.0372,  0.0347,  ...,  0.0539, -0.0839, -0.0111],\n",
      "        [ 0.0135, -0.0038,  0.0024,  ...,  0.0323, -0.0786,  0.0105]],\n",
      "       grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.9.attention.self.value.bias | Size: torch.Size([768]) | Values : tensor([-0.0602, -0.0212], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.9.attention.output.dense.weight | Size: torch.Size([768, 768]) | Values : tensor([[-0.0687, -0.0305, -0.0712,  ..., -0.0119,  0.0574,  0.0215],\n",
      "        [-0.0407,  0.0395, -0.0349,  ...,  0.0006,  0.0370, -0.0690]],\n",
      "       grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.9.attention.output.dense.bias | Size: torch.Size([768]) | Values : tensor([-0.0901, -0.0304], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.9.attention.output.LayerNorm.weight | Size: torch.Size([768]) | Values : tensor([0.8358, 0.8486], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.9.attention.output.LayerNorm.bias | Size: torch.Size([768]) | Values : tensor([ 0.0378, -0.0628], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.9.intermediate.dense.weight | Size: torch.Size([3072, 768]) | Values : tensor([[-0.0350,  0.0414,  0.0051,  ..., -0.0156,  0.0100,  0.0589],\n",
      "        [-0.0131, -0.0104,  0.0743,  ...,  0.0216, -0.0141,  0.0093]],\n",
      "       grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.9.intermediate.dense.bias | Size: torch.Size([3072]) | Values : tensor([-0.0632, -0.0685], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.9.output.dense.weight | Size: torch.Size([768, 3072]) | Values : tensor([[-0.0574, -0.0161, -0.0158,  ..., -0.0349, -0.0458,  0.0557],\n",
      "        [-0.0087, -0.0331,  0.0431,  ...,  0.0249,  0.0450,  0.0231]],\n",
      "       grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.9.output.dense.bias | Size: torch.Size([768]) | Values : tensor([ 0.0802, -0.0253], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.9.output.LayerNorm.weight | Size: torch.Size([768]) | Values : tensor([0.8102, 0.7345], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.9.output.LayerNorm.bias | Size: torch.Size([768]) | Values : tensor([-0.0108, -0.0043], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.10.attention.self.query.weight | Size: torch.Size([768, 768]) | Values : tensor([[-0.0201,  0.0136,  0.0211,  ...,  0.0389,  0.0237,  0.0136],\n",
      "        [-0.0474, -0.0002,  0.0424,  ..., -0.0164,  0.0767, -0.0622]],\n",
      "       grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.10.attention.self.query.bias | Size: torch.Size([768]) | Values : tensor([-0.0087,  0.1030], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.10.attention.self.key.weight | Size: torch.Size([768, 768]) | Values : tensor([[ 0.0647, -0.0184, -0.0262,  ..., -0.0339, -0.1105, -0.0172],\n",
      "        [-0.0246, -0.0595,  0.0498,  ..., -0.0466, -0.0240, -0.0017]],\n",
      "       grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.10.attention.self.key.bias | Size: torch.Size([768]) | Values : tensor([-0.0005, -0.0032], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.10.attention.self.value.weight | Size: torch.Size([768, 768]) | Values : tensor([[-0.0296,  0.0010, -0.0503,  ..., -0.0313,  0.0811, -0.0354],\n",
      "        [ 0.0038, -0.0045,  0.0591,  ..., -0.0498,  0.0242,  0.0168]],\n",
      "       grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.10.attention.self.value.bias | Size: torch.Size([768]) | Values : tensor([ 0.0033, -0.0266], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.10.attention.output.dense.weight | Size: torch.Size([768, 768]) | Values : tensor([[ 0.0212, -0.0260, -0.0075,  ..., -0.0126,  0.0292, -0.0066],\n",
      "        [-0.0433, -0.0251, -0.0111,  ...,  0.0282, -0.0049, -0.0146]],\n",
      "       grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.10.attention.output.dense.bias | Size: torch.Size([768]) | Values : tensor([-0.0697,  0.0995], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.10.attention.output.LayerNorm.weight | Size: torch.Size([768]) | Values : tensor([0.8197, 0.7893], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.10.attention.output.LayerNorm.bias | Size: torch.Size([768]) | Values : tensor([ 0.0215, -0.0356], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.10.intermediate.dense.weight | Size: torch.Size([3072, 768]) | Values : tensor([[ 0.0416,  0.0047,  0.0446,  ..., -0.0174, -0.0282,  0.0649],\n",
      "        [ 0.0208, -0.0415,  0.0274,  ...,  0.0027,  0.0006,  0.0084]],\n",
      "       grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.10.intermediate.dense.bias | Size: torch.Size([3072]) | Values : tensor([-0.0430, -0.0564], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.10.output.dense.weight | Size: torch.Size([768, 3072]) | Values : tensor([[-0.0184,  0.0274, -0.0223,  ...,  0.0225, -0.0495, -0.0044],\n",
      "        [-0.0790, -0.0991, -0.1043,  ...,  0.0453,  0.0485, -0.0081]],\n",
      "       grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.10.output.dense.bias | Size: torch.Size([768]) | Values : tensor([0.1330, 0.0102], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.10.output.LayerNorm.weight | Size: torch.Size([768]) | Values : tensor([0.8359, 0.7870], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.10.output.LayerNorm.bias | Size: torch.Size([768]) | Values : tensor([-0.0045, -0.0060], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.11.attention.self.query.weight | Size: torch.Size([768, 768]) | Values : tensor([[ 0.0085, -0.0470, -0.0122,  ..., -0.0161,  0.0452, -0.0192],\n",
      "        [-0.0711, -0.0427,  0.0291,  ..., -0.0296, -0.1053, -0.0579]],\n",
      "       grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.11.attention.self.query.bias | Size: torch.Size([768]) | Values : tensor([ 0.1233, -0.0460], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.11.attention.self.key.weight | Size: torch.Size([768, 768]) | Values : tensor([[-0.0388,  0.0139,  0.0161,  ..., -0.0820,  0.0342,  0.0480],\n",
      "        [-0.0597, -0.0349,  0.0138,  ..., -0.0405, -0.0854, -0.1586]],\n",
      "       grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.11.attention.self.key.bias | Size: torch.Size([768]) | Values : tensor([-0.0045,  0.0029], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.11.attention.self.value.weight | Size: torch.Size([768, 768]) | Values : tensor([[ 0.0314,  0.0137,  0.0377,  ...,  0.0599, -0.0712, -0.0178],\n",
      "        [-0.0856, -0.0559,  0.0214,  ..., -0.0293, -0.0011,  0.0534]],\n",
      "       grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.11.attention.self.value.bias | Size: torch.Size([768]) | Values : tensor([-0.0415,  0.0087], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.11.attention.output.dense.weight | Size: torch.Size([768, 768]) | Values : tensor([[ 0.0340, -0.0167, -0.0009,  ..., -0.0810,  0.0189, -0.0194],\n",
      "        [-0.0188,  0.0686,  0.0458,  ...,  0.0459,  0.0141, -0.0477]],\n",
      "       grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.11.attention.output.dense.bias | Size: torch.Size([768]) | Values : tensor([-0.0060,  0.0176], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.11.attention.output.LayerNorm.weight | Size: torch.Size([768]) | Values : tensor([0.7644, 0.7214], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.11.attention.output.LayerNorm.bias | Size: torch.Size([768]) | Values : tensor([ 0.0483, -0.0860], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.11.intermediate.dense.weight | Size: torch.Size([3072, 768]) | Values : tensor([[-0.0309, -0.0287,  0.0526,  ...,  0.0017,  0.0101,  0.0223],\n",
      "        [-0.0468,  0.0614,  0.0301,  ..., -0.0867,  0.0702, -0.0459]],\n",
      "       grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.11.intermediate.dense.bias | Size: torch.Size([3072]) | Values : tensor([ 0.0593, -0.0407], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.11.output.dense.weight | Size: torch.Size([768, 3072]) | Values : tensor([[ 0.0738,  0.0221, -0.0160,  ...,  0.0276, -0.0295, -0.0532],\n",
      "        [-0.0259, -0.0066, -0.0252,  ...,  0.0054,  0.0157, -0.0700]],\n",
      "       grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.11.output.dense.bias | Size: torch.Size([768]) | Values : tensor([ 0.0633, -0.0159], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.11.output.LayerNorm.weight | Size: torch.Size([768]) | Values : tensor([0.8946, 0.8758], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.encoder.layer.11.output.LayerNorm.bias | Size: torch.Size([768]) | Values : tensor([ 0.1191, -0.0997], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.pooler.dense.weight | Size: torch.Size([768, 768]) | Values : tensor([[ 0.0131,  0.0129,  0.0299,  ..., -0.0058,  0.0069,  0.0442],\n",
      "        [-0.0174, -0.0249,  0.0064,  ...,  0.0097,  0.0249, -0.0189]],\n",
      "       grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: base_encoder.model.pooler.dense.bias | Size: torch.Size([768]) | Values : tensor([0., 0.], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: term_scorer.linear_softmax.0.weight | Size: torch.Size([3, 768]) | Values : tensor([[-0.0282,  0.0317, -0.0152,  ..., -0.0102,  0.0556,  0.0039],\n",
      "        [-0.0670, -0.0121, -0.0433,  ...,  0.0625, -0.0012, -0.0041]],\n",
      "       grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: term_scorer.linear_softmax.0.bias | Size: torch.Size([3]) | Values : tensor([ 0.0592, -0.0185], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: relation_scorer.span_scorer.0.weight | Size: torch.Size([3, 768]) | Values : tensor([[ 0.0011, -0.0360, -0.0093,  ..., -0.0125,  0.0109, -0.0232],\n",
      "        [ 0.0064,  0.0037, -0.0358,  ...,  0.0047, -0.0292, -0.0138]],\n",
      "       grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: relation_scorer.span_scorer.0.bias | Size: torch.Size([3]) | Values : tensor([-0.0019,  0.0208], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: relation_scorer.pair_scorer.0.weight | Size: torch.Size([3, 2304]) | Values : tensor([[ 0.0314,  0.0032,  0.0292,  ...,  0.0143,  0.0355,  0.0467],\n",
      "        [-0.0071, -0.0411, -0.0034,  ..., -0.0102, -0.0533, -0.0314]],\n",
      "       grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: relation_scorer.pair_scorer.0.bias | Size: torch.Size([3]) | Values : tensor([ 0.0173, -0.0338], grad_fn=<SliceBackward>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Model structure: \", span_mltri, \"\\n\\n\")\n",
    "\n",
    "for name, param in span_mltri.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
